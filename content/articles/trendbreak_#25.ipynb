{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inuugujoq! üá¨üá±\n",
    "\n",
    "Just a few days ago, the code-hosting platform GitHub (acquired by Microsoft a few years back) [announced the general availability of Copilot](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/), its new product developed in partnership with OpenAI. It's a code-writing assistant powered by AI, kind of like autocomplete on steroids, as during tests conducted with over a million developers, it managed to write up to 40% of the code ü§ñüßë‚Äçüíª.<br>\n",
    "Technically, Copilot is built on recent advancements in language generation models using deep neural networks, such as GPT-3. Indeed, a programming language can certainly be considered a language, with its grammar (usually stricter than human languages), idioms, and even aesthetics.<br>\n",
    "The training data for such a model is the other critical component. On this point, as with other models created by Big Tech companies, [questions have been raised](https://www.theverge.com/2021/7/7/22561180/github-copilot-legal-copyright-fair-use-public-code), since Copilot is trained on a massive amount of open-source code. While GitHub assures it filters out projects that declare a license prohibiting code reuse for commercial purposes, it's a gray area when it comes to \"fair use\". Additionally, it appears that 40% of the generated code contains security flaws, raising questions about liability.\n",
    "\n",
    "Chances are you're familiar with scikit-learn, the world's most widely used machine learning library. Intel has developed an open-source extension, [Intelex](https://github.com/intel/scikit-learn-intelex), that dramatically enhances computational performance, slashing the training time of numerous models by 10x or even 100x! üöÄ<br>\n",
    "Intel utilizes many low-level enhancements (optimized processor instruction sets for vector computation, computation libraries like oneMKL) that work on both CPUs and GPUs. You can find the list of supported models [here](https://intel.github.io/scikit-learn-intelex/algorithms.html).\n",
    "\n",
    "We'll wrap up this edition with a [scientific literature review](https://arxiv.org/pdf/2012.00174.pdf), wherein Andrew Gelman (a professor at Columbia University in New York, and one of the main developers of the probabilistic programming language [Stan](https://mc-stan.org/)) and Aki Vehtari (a professor at Aalto University üá´üáÆ) detail the eight most important statistical concepts of the past fifty years. Among them, you'll find causal inference, regularization and overparameterized models, robust inference, and exploratory data analysis.<br>\n",
    "With over 180 references selected by the authors, this article is a good starting point to learn more about these fundamental techniques and ideas.<br>\n",
    "As a bonus, here's a [visualization of these articles](https://time.graphics/line/459681) in the form of a timeline by Anna Menacher, a doctoral student at Oxford.\n",
    "<img src=\"/images/timeline_important_stats_concepts.png\" title='Timeline of the eight most important statistical concepts.' style=\"width:600px\"/>\n",
    "\n",
    "Happy reading and have a great week! ü§ì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
